{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a4a985",
   "metadata": {},
   "source": [
    "# Torch to ONNX Conversion for Inference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec24c73",
   "metadata": {},
   "source": [
    "ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de4544",
   "metadata": {},
   "source": [
    "## Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064a2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets.folder import make_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e18f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8960a",
   "metadata": {},
   "source": [
    "## Data and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9a73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PATH = './custom_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25db28aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model with the pretrained weights\n",
    "model = models.vgg16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(num_ftrs, 5)\n",
    "\n",
    "\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(torch.load(WEIGHT_PATH,map_location=torch.device('cpu')))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0e8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader for testing.\n",
    "test_batch_size = 64\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=4,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7f7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "x = torch.randn(test_batch_size, 3, 224, 224, requires_grad=True)\n",
    "torch_out = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc376ea4",
   "metadata": {},
   "source": [
    "# ONNX Export testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3df855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3dd19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"./onnx_converted.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc3848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./onnx_converted.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "826799c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class sg_food_dataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, root, class_id, transform=None):\n",
    "        self.class_id = class_id\n",
    "        self.root = root\n",
    "        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n",
    "        if not all_classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "        self.classes = [all_classes[x] for x in class_id]\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        self.samples = make_dataset(self.root, self.class_to_idx, extensions=('jpg'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        with open(path, \"rb\") as f:\n",
    "            sample = Image.open(f).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a60b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbec7442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected classes:\n",
      "    id: [3, 5, 7, 8, 9]\n",
      "    name: ['Hokkien Prawn Mee', 'Laksa', 'Oyster Omelette', 'Roast Meat Rice', 'Roti Prata']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data augmentation and normalization for training\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # Define data preparation operations for training set here.\n",
    "        # Tips: Use torchvision.transforms\n",
    "        #       https://pytorch.org/vision/stable/transforms.html\n",
    "        #       Normally this should at least contain resizing (Resize) and data format converting (ToTensor).\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        # Define data preparation operations for testing/validation set here.\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.path.join('./', 'sg_food')\n",
    "subfolder = {'train': 'train', 'val': 'val'}\n",
    "\n",
    "# Define the dataset\n",
    "selected_classes = [3,5,7,8,9]\n",
    "n_classes = len(selected_classes)\n",
    "image_datasets = {x: sg_food_dataset(root=os.path.join(data_dir, subfolder[x]),\n",
    "                                     class_id=selected_classes,\n",
    "                                     transform=data_transforms[x]) \n",
    "                  for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72fcb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = './sg_food/test'\n",
    "\n",
    "# Define the test set.\n",
    "test_dataset = sg_food_dataset(root=test_dir, class_id=selected_classes, transform=data_transforms['val'])\n",
    "test_sizes = len(test_dataset)\n",
    "\n",
    "# Define the dataloader for testing.\n",
    "test_batch_size = 64\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1d9a95e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "----------\n",
      "Eval time...\n",
      "125.17501211166382\n",
      "Testing Acc: 0.8318\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "\n",
    "print('Evaluation')\n",
    "print('-' * 10)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "wrong_detections = []\n",
    "correct_detections = []\n",
    "\n",
    "time_total= 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate over the testing dataset.\n",
    "    for (inputs, labels) in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(inputs)}\n",
    "        time_start = time.time()\n",
    "        ort_outs = ort_session.run(None, ort_inputs)\n",
    "        time_total += time.time() - time_start\n",
    "#         print(type(ort_outs))\n",
    "#         print(len(ort_outs[0]))\n",
    "        \n",
    "        # Predict on the test set\n",
    "        # outputs = model(inputs)\n",
    "        output_tensors = torch.FloatTensor(ort_outs[0])\n",
    "        _, preds = torch.max(output_tensors, 1)\n",
    "#         print(preds)\n",
    "        # preds = ort_outs\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        \n",
    "        y_true.extend(preds.numpy())\n",
    "        y_pred.extend(labels.data.numpy())\n",
    "        \n",
    "        test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "print('Eval time...')\n",
    "print(time_total)\n",
    "# Compute the testing accuracy\n",
    "test_acc = test_acc.double() / test_sizes\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26c507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797c489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "230554f6",
   "metadata": {},
   "source": [
    "## Torch based inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53fc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "----------\n",
      "Eval time...\n",
      "208.15612721443176\n",
      "Testing Acc: 0.8318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_acc = 0\n",
    "\n",
    "print('Evaluation')\n",
    "print('-' * 10)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "wrong_detections = []\n",
    "correct_detections = []\n",
    "\n",
    "time_now= time.time()\n",
    "\n",
    "\n",
    "time_total= 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate over the testing dataset.\n",
    "    for (inputs, labels) in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        # Predict on the test set\n",
    "        time_start = time.time()\n",
    "        outputs = model(inputs)\n",
    "        time_total += time.time() - time_start\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        preds = preds.cpu()\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        \n",
    "        y_true.extend(preds.numpy())\n",
    "        y_pred.extend(labels.data.numpy())\n",
    "        \n",
    "        test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "print('Eval time...')\n",
    "print(time.time()-time_now)\n",
    "# Compute the testing accuracy\n",
    "test_acc = test_acc.double() / test_sizes\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750d61c",
   "metadata": {},
   "source": [
    "# Benchmarking Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205e9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f976bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e236a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(test_batch_size, 3, 224, 224, requires_grad=True)\n",
    "data = np.random.rand(test_batch_size,3,224,224).astype(np.float32)\n",
    "torch_data = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f3806a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_inf():\n",
    "    model(torch_data)\n",
    "\n",
    "def onnx_inf():\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: data}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58b18a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849be5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_t = timeit(lambda : torch_inf(), number=n)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0cfbb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.876783071699998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb7363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 12.876783071699998 VS ONNX 7.9055955048500035\n"
     ]
    }
   ],
   "source": [
    "onnx_t = timeit(lambda : onnx_inf(), number=n)/20\n",
    "\n",
    "print(f\"PyTorch {torch_t} VS ONNX {onnx_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b4098d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "frameworks = [\"PyTorch\", \"ONNX\"]\n",
    "times = [torch_t, onnx_t]\n",
    "\n",
    "plt.bar(frameworks[0], times[0])\n",
    "plt.bar(frameworks[1], times[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "818f4e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3860581900906168"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch_t-onnx_t)/torch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f57f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39588577797421165"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(208.156-125.75)/208.156"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187dd72",
   "metadata": {},
   "source": [
    "# Testing Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b0c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"./deployment/onnx_deployment.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "ort_session = onnxruntime.InferenceSession(\"./deployment/onnx_deployment.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfade4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('./Roti Prata.jpg',0)\n",
    "dim = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b59f7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize image\n",
    "resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3026b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91781ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8397df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc50e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = to_numpy(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87171cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7590547 ,  0.74193   ,  0.7590547 , ..., -0.6622999 ,\n",
       "        -0.6109256 , -0.5424266 ],\n",
       "       [ 0.7590547 ,  0.7590547 ,  0.7761795 , ..., -0.57667613,\n",
       "        -0.4054286 , -0.33692956],\n",
       "       [ 0.7590547 ,  0.74193   ,  0.7590547 , ..., -0.30268008,\n",
       "        -0.14855729, -0.09718303],\n",
       "       ...,\n",
       "       [ 1.3070468 ,  0.9816765 ,  0.5878072 , ..., -0.04580877,\n",
       "        -0.6109256 , -0.79929787],\n",
       "       [ 0.9988013 ,  0.6563062 ,  0.33093593, ...,  0.2624369 ,\n",
       "        -0.45680285, -0.79929787],\n",
       "       [ 0.6563062 ,  0.57068247,  0.2966864 , ...,  0.63918144,\n",
       "        -0.16568205, -0.6280504 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7beb837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164., 165., 166., ..., 255., 255., 255.],\n",
       "       [165., 164., 164., ..., 255., 255., 255.],\n",
       "       [165., 164., 164., ..., 255., 255., 255.],\n",
       "       ...,\n",
       "       [217., 216., 216., ..., 255., 255., 255.],\n",
       "       [218., 217., 217., ..., 255., 255., 255.],\n",
       "       [218., 218., 217., ..., 255., 255., 255.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "589ac8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy_dataset = sg_food_dataset(root='./deploy_test', class_id=selected_classes, transform=data_transforms['val'])\n",
    "# deploy_batch_size = 1\n",
    "# deploy_loader = torch.utils.data.DataLoader(deploy_dataset, batch_size=deploy_batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2461b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "142a2396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6431373 , 0.64705884, 0.6509804 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.64705884, 0.6431373 , 0.6431373 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.64705884, 0.6431373 , 0.6431373 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.8509804 , 0.84705883, 0.84705883, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.85490197, 0.8509804 , 0.8509804 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.85490197, 0.85490197, 0.8509804 , ..., 1.        , 1.        ,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13ab1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: resized_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb1be24f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input Got: 2 Expected: 4 Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7813/3449483130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mort_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mort_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input Got: 2 Expected: 4 Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5d0c791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1940864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('./sg_food/test/Hokkien Prawn Mee/Hokkien Prawn Mee(28).jpg',cv2.COLOR_BGR2RGB)\n",
    "f = './sg_food/test/Hokkien Prawn Mee/Hokkien Prawn Mee(28).jpg'\n",
    "img = Image.open(f).convert('RGB')\n",
    "transform_img = transforms.Compose([\n",
    "        # Define data preparation operations for testing/validation set here.\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n",
    "    ])\n",
    "input_img = transform_img(img)\n",
    "input_img = input_img.unsqueeze_(0)\n",
    "input_img =input_img.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd7e0679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99a7d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_img)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a18141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13098d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 5.582136 ,  5.463295 , -6.272268 , -0.6216311, -4.644254 ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3796a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensors = torch.FloatTensor(ort_outs[0])\n",
    "_, preds = torch.max(output_tensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49cdb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4477df34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hokkien Prawn Mee'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c69ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
